{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnoIJZyUXHTA"
      },
      "source": [
        "# **Predicción de sentimientos de Análisis de Reseñas de Libros en Amazon - Utilización, procesamiento y visualización de grandes volúmenes de datos (Portafolio Análisis)**\n",
        "\n",
        "Gabriela Chimali Nava Ramírez - A01710530\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CvXAWu_j_83"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "Este proyecto aborda un desafío de Big Data, analizar 3 millones de reseñas de libros de Amazon, un conjunto de datos de más de 2.6 GB. El objetivo principal es construir y evaluar un modelo de Machine Learning capaz de clasificar automáticamente el sentimiento de una reseña como \"Positivo\" (1.0) o \"Negativo\" (0.0).\n",
        "\n",
        "Para manejar este volumen de datos, todo el proceso se realiza con Apache Spark (PySpark). El cuaderno documenta el flujo de trabajo completo:\n",
        "\n",
        "1. ETL: Carga, limpieza y unión de los datos de reseñas y metadatos de los libros.\n",
        "\n",
        "2. Feature Engineering: Conversión del texto de las reseñas (NLP) y las categorías de los libros en características numéricas que el modelo pueda entender.\n",
        "\n",
        "3. Modelado: Se entrenan dos modelos, Regresión Logística y Random Forest, para comparar su rendimiento. Utilizando una técnica de ponderación de clases (weightCol) para ayudar al Random Forest a manejar el desbalance entre reseñas positivas y negativas.\n",
        "\n",
        "4. Evaluación: Se evalúan los modelos usando métricas como Precisión, Recall y F1-Score.\n",
        "\n",
        "5. Visualización: Se exportan los resultados agregados para su visualización en Tableau."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5RXxBLH7iJI"
      },
      "source": [
        "## 0. Inicializar entorno PySpark y montar Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EknDcIlgYMRN"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "!pip install pyspark -q\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.12/dist-packages/pyspark\"\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ProjectBigData\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr63cCy5hU3P",
        "outputId": "e09e3913-7f04-4820-f000-59062ed3ad5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqCZQXwGDasy"
      },
      "source": [
        "## 1. ETL inicial de los datos\n",
        "\n",
        "### 1.1. Carga de DataFrames\n",
        "\n",
        "El análisis utiliza dos archivos del dataset \"Amazon Books Reviews\" de Kaggle:\n",
        "\n",
        "1. `Books_rating.csv` (2.64 GB): Contiene aproximadamente 3 millones de reseñas. Las columnas clave utilizadas son:\n",
        "* Title: Título del libro (usado como clave de unión).\n",
        "* review/score: Calificación numérica (1.0 a 5.0) dada por el usuario.\n",
        "* review/text: El texto completo de la reseña.\n",
        "\n",
        "2. `books_data.csv` (455 MB): Contiene metadatos de los libros. Las columnas clave utilizadas son:\n",
        "* Title: Título del libro (usado como clave de unión).\n",
        "* categories: Género o categoría del libro.\n",
        "* authors: Autor(es) del libro.\n",
        "* publishedDate: Fecha de publicación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3XwxskD36X6",
        "outputId": "0f77a5ad-c303-4ae5-80cd-4a5386d770b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Cargando DataFrames...\n"
          ]
        }
      ],
      "source": [
        "# Rutas a los archivos\n",
        "ratings_path = \"/content/gdrive/MyDrive/AI_data_set/AmazonBooksReviews/Books_rating.csv\"\n",
        "metadata_path = \"/content/gdrive/MyDrive/AI_data_set/AmazonBooksReviews/books_data.csv\"\n",
        "\n",
        "print(\"\\nCargando DataFrames...\")\n",
        "df_ratings_raw = spark.read.csv(\n",
        "    ratings_path,\n",
        "    header=True,\n",
        "    inferSchema=True,\n",
        "    sep=',',\n",
        "    multiLine=True,\n",
        "    escape='\"'\n",
        ")\n",
        "\n",
        "df_books_raw = spark.read.csv(\n",
        "    metadata_path,\n",
        "    header=True,\n",
        "    inferSchema=True,\n",
        "    sep=',',\n",
        "    multiLine=True,\n",
        "    escape='\"'\n",
        ")\n",
        "\n",
        "# Seleccionar sólo las columnas necesarias\n",
        "df_ratings = df_ratings_raw.select(\n",
        "    col(\"Title\").alias(\"title\"),\n",
        "    col(\"review/score\").alias(\"reviewScore\"),\n",
        "    col(\"review/text\").alias(\"reviewText\")\n",
        ")\n",
        "\n",
        "df_books = df_books_raw.select(\n",
        "    col(\"Title\").alias(\"title\"),\n",
        "    \"categories\",\n",
        "    \"authors\",\n",
        "    \"publishedDate\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP4x4PCQNgZx",
        "outputId": "3ea09876-7144-4825-c9d4-c4e08a3c3953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reseñas cargadas: 3000000\n",
            "Tamaño del archivo de reseñas: 2.66 GB\n",
            "Datos de los libros cargados: 212404\n",
            "Tamaño del archivo de datos de libros: 0.17 GB\n"
          ]
        }
      ],
      "source": [
        "def file_size(file_path):\n",
        "    size_bytes = os.path.getsize(file_path)\n",
        "    size_gb = size_bytes / (1024 * 1024 * 1024)\n",
        "    return size_gb\n",
        "\n",
        "print(f\"Reseñas cargadas: {df_ratings.count()}\")\n",
        "print(f\"Tamaño del archivo de reseñas: {file_size(ratings_path):.2f} GB\")\n",
        "print(f\"Datos de los libros cargados: {df_books.count()}\")\n",
        "print(f\"Tamaño del archivo de datos de libros: {file_size(metadata_path):.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FddciWzgOifK",
        "outputId": "c10fc8d7-b1a7-497f-fa6b-5f813fc2e765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Primeras 10 filas de df_ratings:\n",
            "+--------------------+-----------+--------------------+\n",
            "|               title|reviewScore|          reviewText|\n",
            "+--------------------+-----------+--------------------+\n",
            "|Its Only Art If I...|        4.0|This is only for ...|\n",
            "|Dr. Seuss: Americ...|        5.0|I don't care much...|\n",
            "|Dr. Seuss: Americ...|        5.0|If people become ...|\n",
            "|Dr. Seuss: Americ...|        4.0|Theodore Seuss Ge...|\n",
            "|Dr. Seuss: Americ...|        4.0|Philip Nel - Dr. ...|\n",
            "|Dr. Seuss: Americ...|        4.0|\"Dr. Seuss: Ameri...|\n",
            "|Dr. Seuss: Americ...|        5.0|Theodor Seuss Gie...|\n",
            "|Dr. Seuss: Americ...|        5.0|When I recieved t...|\n",
            "|Dr. Seuss: Americ...|        5.0|Trams (or any pub...|\n",
            "|Dr. Seuss: Americ...|        4.0|As far as I am aw...|\n",
            "+--------------------+-----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "\n",
            "Primeras 10 filas de df_books:\n",
            "+--------------------+--------------------+--------------------+-------------+\n",
            "|               title|          categories|             authors|publishedDate|\n",
            "+--------------------+--------------------+--------------------+-------------+\n",
            "|Its Only Art If I...|['Comics & Graphi...|    ['Julie Strain']|         1996|\n",
            "|Dr. Seuss: Americ...|['Biography & Aut...|      ['Philip Nel']|   2005-01-01|\n",
            "|Wonderful Worship...|        ['Religion']|    ['David R. Ray']|         2000|\n",
            "|Whispers of the W...|         ['Fiction']| ['Veronica Haddon']|      2005-02|\n",
            "|Nation Dance: Rel...|                NULL|     ['Edward Long']|   2003-03-01|\n",
            "|The Church of Chr...|        ['Religion']|['Everett Ferguson']|         1996|\n",
            "|The Overbury affa...|                NULL|['Miriam Allen De...|         1960|\n",
            "|A Walk in the Woo...|                NULL|    ['Lee Blessing']|         1988|\n",
            "|Saint Hyacinth of...|['Biography & Aut...|['Mary Fabyan Win...|   2009-01-01|\n",
            "|Rising Sons and D...|  ['Social Science']|  ['Steven Wardell']|         1995|\n",
            "+--------------------+--------------------+--------------------+-------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nPrimeras 10 filas de df_ratings:\")\n",
        "df_ratings.show(10)\n",
        "\n",
        "print(\"\\nPrimeras 10 filas de df_books:\")\n",
        "df_books.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJAuITsb7K7k"
      },
      "source": [
        "### 1.2. Limpieza de los datos para el JOIN\n",
        "\n",
        "Analizando las gráficas de previsualización en Kaggle y algunos trabajos de la plataforma (Revisar sección de *Referencias*), parece ser que hay registros vacíos en columnas necesarias para la unión de los *datasets*, la puntución para el etiquetado posterior y el texto para el procesamiento de lenguaje natural (NLP). Es por esto que se corrobora su existencia para proceder con la limpieza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edj5maPk4XWa",
        "outputId": "5822adad-adf4-47bb-b015-0ddaf4d90f1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registros nulos en df_books:\n",
            "+-----+----------+-------+-------------+\n",
            "|title|categories|authors|publishedDate|\n",
            "+-----+----------+-------+-------------+\n",
            "|    1|     41199|  31413|        25305|\n",
            "+-----+----------+-------+-------------+\n",
            "\n",
            "Registros nulos en df_ratings:\n",
            "+-----+-----------+----------+\n",
            "|title|reviewScore|reviewText|\n",
            "+-----+-----------+----------+\n",
            "|  208|          0|         8|\n",
            "+-----+-----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Registros nulos en df_books:\")\n",
        "df_books.select([\n",
        "    count(when(isnull(col) | isnan(col), col)).alias(col)\n",
        "    for col in df_books.columns\n",
        "]).show()\n",
        "\n",
        "print(\"Registros nulos en df_ratings:\")\n",
        "df_ratings.select([\n",
        "    count(when(isnull(col) | isnan(col), col)).alias(col)\n",
        "    for col in df_ratings.columns\n",
        "]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDIHL6OfULBa"
      },
      "source": [
        "Se **eliminan** (`dropna`) las filas si las columnas críticas son nulas.\n",
        "* Para ratings: `[\"title\", \"reviewScore\", \"reviewText\"]`\n",
        "* Para books: `[\"title\"]`\n",
        "\n",
        "Se **imputan** (`fillna`) los nulos de columnas de books `[\"categories\", \"authors\"]` con el valor `\"Desconocido\"`. De esta forma podríamos tratar este valor como una categoría en sí. Por otro lado, para `[\"publicationDate\"]` se imputa `0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQRJBRpwUFbv",
        "outputId": "a546a365-5b97-4daf-9e6a-ee16698f60ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reseñas después de limpieza: 2999784\n",
            "Books después de limpieza: 212403\n"
          ]
        }
      ],
      "source": [
        "# Eliminar filas de ratings\n",
        "columnas_criticas_ratings = [\"title\", \"reviewScore\", \"reviewText\"]\n",
        "df_ratings_clean = df_ratings.dropna(subset=columnas_criticas_ratings)\n",
        "\n",
        "# Eliminar filas de books si \"Title\" es nulo\n",
        "df_books_clean = df_books.dropna(subset=[\"title\"])\n",
        "\n",
        "# Imputar features nulos en books\n",
        "columnas_imputar = [\"categories\", \"authors\"]\n",
        "df_books_clean = df_books_clean.fillna(\"Desconocido\", subset=columnas_imputar)\n",
        "\n",
        "df_books_clean = df_books_clean.withColumn(\n",
        "    \"publicationYear\",\n",
        "    substring(col(\"publishedDate\"), 1, 4).cast(IntegerType())\n",
        ")\n",
        "# Imputar años nulos o mal formados con 0 en books\n",
        "df_books_clean = df_books_clean.fillna(0, subset=[\"publicationYear\"])\n",
        "df_books_clean = df_books_clean.drop(\"publishedDate\")\n",
        "\n",
        "print(f\"Reseñas después de limpieza: {df_ratings_clean.count()}\")\n",
        "print(f\"Books después de limpieza: {df_books_clean.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se aplica `lower, trim y regexp_replace` a la columna `[\"title\"]` en ambos *DataFrames* para estandarizar la clave de unión. Sin esta normalización, un mismo libro con variaciones en la escritura del título no coincidiría. Por ejemplo: \"mi libro\", \"Mi Libro.\", etcétera; resultando en un join fallido y pérdida de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "djC0ZL9PXinr"
      },
      "outputs": [],
      "source": [
        "df_books_clean = df_books_clean.withColumn(\n",
        "    \"title\",\n",
        "    trim(regexp_replace(lower(col(\"title\")), r\"[^a-z0-9\\s]\", \"\"))\n",
        ")\n",
        "\n",
        "df_ratings_clean = df_ratings_clean.withColumn(\n",
        "    \"title\",\n",
        "    trim(regexp_replace(lower(col(\"title\")), r\"[^a-z0-9\\s]\", \"\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfT5GU_Qgr6G",
        "outputId": "df418449-e609-4b87-feed-6da3ff0776b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+---------------+\n",
            "|               title|          categories|             authors|publicationYear|\n",
            "+--------------------+--------------------+--------------------+---------------+\n",
            "|its only art if i...|['Comics & Graphi...|    ['Julie Strain']|           1996|\n",
            "|dr seuss american...|['Biography & Aut...|      ['Philip Nel']|           2005|\n",
            "|wonderful worship...|        ['Religion']|    ['David R. Ray']|           2000|\n",
            "|whispers of the w...|         ['Fiction']| ['Veronica Haddon']|           2005|\n",
            "|nation dance reli...|         Desconocido|     ['Edward Long']|           2003|\n",
            "|the church of chr...|        ['Religion']|['Everett Ferguson']|           1996|\n",
            "|the overbury affa...|         Desconocido|['Miriam Allen De...|           1960|\n",
            "|a walk in the woo...|         Desconocido|    ['Lee Blessing']|           1988|\n",
            "|saint hyacinth of...|['Biography & Aut...|['Mary Fabyan Win...|           2009|\n",
            "|rising sons and d...|  ['Social Science']|  ['Steven Wardell']|           1995|\n",
            "+--------------------+--------------------+--------------------+---------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+--------------------+-----------+--------------------+\n",
            "|               title|reviewScore|          reviewText|\n",
            "+--------------------+-----------+--------------------+\n",
            "|its only art if i...|        4.0|This is only for ...|\n",
            "|dr seuss american...|        5.0|I don't care much...|\n",
            "|dr seuss american...|        5.0|If people become ...|\n",
            "|dr seuss american...|        4.0|Theodore Seuss Ge...|\n",
            "|dr seuss american...|        4.0|Philip Nel - Dr. ...|\n",
            "|dr seuss american...|        4.0|\"Dr. Seuss: Ameri...|\n",
            "|dr seuss american...|        5.0|Theodor Seuss Gie...|\n",
            "|dr seuss american...|        5.0|When I recieved t...|\n",
            "|dr seuss american...|        5.0|Trams (or any pub...|\n",
            "|dr seuss american...|        4.0|As far as I am aw...|\n",
            "+--------------------+-----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_books_clean.show(10)\n",
        "df_ratings_clean.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Durante la primer ejecución del join aumentó la cantidad de registros de reseñas, por lo que se deduce que hay títulos duplicados. Estos deben **eliminarse** también pues pueden generar ambigüedad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY174cSHCPcK",
        "outputId": "5ac16c30-e284-4c83-f7f2-08ec25b2967c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Books después de borrar duplicados: 207065\n"
          ]
        }
      ],
      "source": [
        "df_books_clean = df_books_clean.dropDuplicates([\"title\"])\n",
        "print(f\"Books después de borrar duplicados: {df_books_clean.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2wQCoHPIailc"
      },
      "outputs": [],
      "source": [
        "df_join = df_ratings_clean.join(\n",
        "    df_books_clean,\n",
        "    on=\"title\",\n",
        "    how=\"left\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verificación final de presencia de registros nulos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6xZR1eLjxlK",
        "outputId": "5b9ef95c-369a-4aa0-ec8b-deda74df762c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registros nulos en df_join:\n",
            "+-----+-----------+----------+----------+-------+---------------+\n",
            "|title|reviewScore|reviewText|categories|authors|publicationYear|\n",
            "+-----+-----------+----------+----------+-------+---------------+\n",
            "|    0|          0|         0|         0|      0|              0|\n",
            "+-----+-----------+----------+----------+-------+---------------+\n",
            "\n",
            "Reseñas después del join: 2999784\n"
          ]
        }
      ],
      "source": [
        "print(\"Registros nulos en df_join:\")\n",
        "df_join.select([\n",
        "    count(when(isnull(col) | isnan(col), col)).alias(col)\n",
        "    for col in df_join.columns\n",
        "]).show()\n",
        "\n",
        "print(f\"Reseñas después del join: {df_join.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae1BPzoAI5JY"
      },
      "source": [
        "### 1.3. Creación de la Etiqueta Binaria para el modelo\n",
        "\n",
        "El objetivo es un modelo de sentimiento (Positivo/Negativo). Así que usando la columna `[\"reviewScore\"]` se genera una etiqueta binaria:\n",
        "* Las reseñas de 4.0 y 5.0 = `1.0` (Positivo).\n",
        "* Las reseñas de 1.0 y 2.0 = `0.0` (Negativo).\n",
        "\n",
        "Se **eliminan** las reseñas de 3.0 estrellas. Ya que no representan gran porcentaje del total de registros y pueden ser ambiguas, neutrales o mixtas (\"El libro es bueno, pero el envío fue malo\"). Incluirlas podría dificultar al modelo aprender la diferencia clara entre positivo y negativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhUNwyS9IHrR",
        "outputId": "6b1a4ac7-d226-4d7d-f3c7-28aeab5ea367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filas para el modelo (sin 3 estrellas): 2745497\n"
          ]
        }
      ],
      "source": [
        "df_labeled = df_join.withColumn(\"label\",\n",
        "    when(col(\"reviewScore\") >= 4.0, 1.0)\n",
        "    .when(col(\"reviewScore\") <= 2.0, 0.0)\n",
        "    .otherwise(None)\n",
        ")\n",
        "\n",
        "df_model_data = df_labeled.dropna(subset=[\"label\"])\n",
        "print(f\"Filas para el modelo (sin 3 estrellas): {df_model_data.count()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isK5QjgzQYor"
      },
      "source": [
        "### 1.4. Limpieza de cadenas de texto\n",
        "\n",
        "Se **eliminan** los caracteres especiales de las columnas `[\"category\"] y [\"authors\"]` y se **queda** sólo la/el primer nombre registrado en la fila, asumiendo que es lo más relevante. Ya que, manejar listas múltiples requeriría técnicas más complejas como multi-hot encoding, lo cual incrementaría la dimensionalidad y costo computacional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtRMSV0fON16",
        "outputId": "f513f581-457a-45c1-edcc-8cf6ad26afc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----------+--------------------+--------------------+----------+---------------+-----+\n",
            "|               title|reviewScore|          reviewText|          categories|   authors|publicationYear|label|\n",
            "+--------------------+-----------+--------------------+--------------------+----------+---------------+-----+\n",
            "|dr seuss american...|        5.0|I don't care much...|Biography & Autob...|Philip Nel|           2005|  1.0|\n",
            "|dr seuss american...|        5.0|If people become ...|Biography & Autob...|Philip Nel|           2005|  1.0|\n",
            "|dr seuss american...|        4.0|Theodore Seuss Ge...|Biography & Autob...|Philip Nel|           2005|  1.0|\n",
            "|dr seuss american...|        4.0|Philip Nel - Dr. ...|Biography & Autob...|Philip Nel|           2005|  1.0|\n",
            "|dr seuss american...|        4.0|\"Dr. Seuss: Ameri...|Biography & Autob...|Philip Nel|           2005|  1.0|\n",
            "+--------------------+-----------+--------------------+--------------------+----------+---------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Limpiar y obtener la primera categoría\n",
        "df_model_data = df_model_data.withColumn(\"categories\",\n",
        "    regexp_replace(col(\"categories\"), r\"[\\'\\[\\]]\", \"\")\n",
        ")\n",
        "df_model_data = df_model_data.withColumn(\"categories\",\n",
        "    split(col(\"categories\"), \",\")[0]\n",
        ")\n",
        "\n",
        "# Limpiar y obtener el primer autor\n",
        "df_model_data = df_model_data.withColumn(\"authors\",\n",
        "    regexp_replace(col(\"authors\"), r\"[\\'\\[\\]]\", \"\")\n",
        ")\n",
        "df_model_data = df_model_data.withColumn(\"authors\",\n",
        "    split(col(\"authors\"), \",\")[0]\n",
        ")\n",
        "\n",
        "df_model_data.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIRuJc5UWetz",
        "outputId": "414c668c-4cc6-4cbb-dada-ce6901ef3de4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> Total de filas con strings limpios: 2745497\n"
          ]
        }
      ],
      "source": [
        "df_complete = df_model_data.select(\n",
        "    \"label\",\n",
        "    \"reviewText\",\n",
        "    col (\"categories\").alias (\"category\"),\n",
        "    col(\"authors\").alias (\"first_author\"),\n",
        "    \"publicationYear\"\n",
        ")\n",
        "\n",
        "df_complete_clean = df_complete.dropna()\n",
        "\n",
        "print(f\">> Total de filas con strings limpios: {df_complete_clean.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4yasVCVhAp1"
      },
      "source": [
        "### 1.5. Seleccionar sólo las categorías más representativas\n",
        "\n",
        "Se conservan únicamente las categorías que representan más del **1% del total de reseñas**, pues las categorías con muy pocas muestras actúan como ruido. Ek modelo tendría dificultades para aprender patrones de categorías que aparecen unas pocas veces, y esto podría llevar a *overfitting*. Este filtrado enfoca al modelo en las categorías más significativas.\n",
        "\n",
        "1. Calcular cantidad de reseñas.\n",
        "2. Calcular cantidad de reseñas por categoría y su equivalencia en porcentaje respecto al total.\n",
        "3. Seleccionar sólo las categorías que representan más del 1% del total.\n",
        "\n",
        "Al final resulta en un archivo de 1.53 GB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af2b4dfa",
        "outputId": "6e13a687-ec64-4c51-85c6-63566a0a3b14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Categorías que representan más del 1% del total de reseñas:\n",
            "['Computers', 'Social Science', 'Biography & Autobiography', 'Business & Economics', 'Religion', 'Fiction', 'History', 'Juvenile Nonfiction', 'Self-Help', 'Juvenile Fiction', 'Desconocido', 'Cooking']\n",
            "\n",
            ">> Filas después de filtrar por categorías (> 1%): 1922939\n"
          ]
        }
      ],
      "source": [
        "total_reviews = df_complete_clean.count()\n",
        "\n",
        "df_category_counts = df_complete_clean.groupBy(\"category\").count()\n",
        "df_category_counts = df_category_counts.withColumn(\n",
        "    \"percentage\",\n",
        "    (col(\"count\") / total_reviews) * 100\n",
        ")\n",
        "\n",
        "categories_to_keep = [row[\"category\"] for row in df_category_counts.filter(col(\"percentage\") > 1).collect()]\n",
        "\n",
        "print(\"Categorías que representan más del 1% del total de reseñas:\")\n",
        "print(categories_to_keep)\n",
        "\n",
        "df_filtered = df_complete_clean.filter(col(\"category\").isin(categories_to_keep))\n",
        "\n",
        "print(f\"\\n>> Filas después de filtrar por categorías (> 1%): {df_filtered.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El paso final del ETL es guardar el *DataFrame* limpio como un archivo CSV en *Google Drive*. Para preservar los datos para su uso posterior sin la necesidad de ejecutar nuevamente todo el cóigo anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-_vbXkLm5nK",
        "outputId": "b2a11109-236e-442a-ea26-70ba12f9b9e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archivo limpio guardado en: /content/gdrive/MyDrive/AI_data_set/AmazonBooksReviews/df_clean\n"
          ]
        }
      ],
      "source": [
        "path_clean = \"/content/gdrive/MyDrive/AI_data_set/AmazonBooksReviews/df_clean\"\n",
        "df_filtered.coalesce(1).write.csv(path_clean, header=True, mode=\"overwrite\")\n",
        "print(f\"Archivo limpio guardado en: {path_clean}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjjMLR3kYkfI"
      },
      "source": [
        "## 2. Feature Engineering\n",
        "\n",
        "### 2.1. Importar librerías y división de datos\n",
        "\n",
        "Separar los conjuntos de entrenamiento (80%) y prueba (20%).\n",
        "\n",
        "Se guardarán en el caché (memoria) para un acceso más rápido. Esto evita recálculos repetitivos de Spark pues este, cada vez que se usa un DataFrame, recalcularía toda su ascendencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHTMUDM3YsMf",
        "outputId": "b6dd2672-defb-4925-a92d-7cde838ce3e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[label: double, reviewText: string, category: string, first_author: string, publicationYear: int]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import (\n",
        "    Tokenizer, StopWordsRemover, HashingTF, IDF,\n",
        "    StringIndexer, OneHotEncoder, VectorAssembler\n",
        ")\n",
        "\n",
        "(train, test) = df_filtered.randomSplit([0.8, 0.2], seed=42)\n",
        "train.cache()\n",
        "test.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0trV741BcFMJ"
      },
      "source": [
        "### 2.2. Procesamiento de Texto (NLP)\n",
        "\n",
        "* Tokenizar: Separar el texto en palabras.\n",
        "\n",
        "* StopWordsRemover: Eliminar palabras comunes que no aportan al modelo. Como \"el\", \"la\", \"y\".\n",
        "\n",
        "* HashingTF: Convertir las palabras en un vector de números."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pZQlgOmYcLNa"
      },
      "outputs": [],
      "source": [
        "tokenizer_text = Tokenizer(inputCol=\"reviewText\", outputCol=\"words_text\")\n",
        "stopwords_text = StopWordsRemover(inputCol=\"words_text\", outputCol=\"filtered_text\")\n",
        "hashingtf_text = HashingTF(inputCol=\"filtered_text\", outputCol=\"features_text\", numFeatures=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvXN6fpodj44"
      },
      "source": [
        "### 2.3. Procesamiento de variables categóricas\n",
        "\n",
        "* StringIndexer: Convierte las categorías en un número.\n",
        "\n",
        "* OneHotEncoder: Convierte en un vector de 1's y 0's ls indices anteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LNPhT7xJdi7z"
      },
      "outputs": [],
      "source": [
        "index_category = StringIndexer(inputCol=\"category\", outputCol=\"category_index\", handleInvalid=\"keep\")\n",
        "ohe_category = OneHotEncoder(inputCol=\"category_index\", outputCol=\"category_vec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqLVir_ne4Af"
      },
      "source": [
        "### 2.4. Ensamblar todas las features\n",
        "\n",
        "Combinación de las columnas en un sólo vector de características que un modelo de machine learning puede procesar. Esto es requerido por los algoritmos de *PySpark ML*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1lAAv93xe9xM"
      },
      "outputs": [],
      "source": [
        "assembler = VectorAssembler(\n",
        "    inputCols=[\n",
        "        \"features_text\",\n",
        "        \"category_vec\",\n",
        "        \"publicationYear\"\n",
        "    ],\n",
        "    outputCol=\"features\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjFz-ER5f1WZ"
      },
      "source": [
        "## 3. Modelado\n",
        "\n",
        "### 3.1. Definir modelo y pipelines\n",
        "\n",
        "Se implementan dos modelos de clasificación para comparar su desempeño:\n",
        "\n",
        "1. **Regresión Logística**: Servirá como punto de referencia. Es simple y rápido de entrenar, pero no incluye manejo especial para el desbalance de clases.\n",
        "\n",
        "2. **Random Forest**: Un modelo más robusto que utiliza un conjunto de árboles de decisión. Para manejar el desbalance significativo en los datos, se implementa una estrategia de pesos de clase (`class_weights`), dando 6 veces más importancia a las reseñas negativas (clase minoritaria).\n",
        "\n",
        "Los modelos se integran en pipelines completos que incluyen todo el preprocesamiento necesario. Para garantizar que tanto los datos de entrenamiento como los de prueba pasen por las mismas transformaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mLcyM0X2geDu"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import (\n",
        "    LogisticRegression, RandomForestClassifier\n",
        ")\n",
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# Modelo 1: Regresión Logística\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Modelo 2: Random Forest\n",
        "class_weights = {1.0: 1.0, 0.0: 6.0}\n",
        "\n",
        "# Agregar columna de \"weight\" al DataFrame\n",
        "df_weighted = df_filtered.withColumn(\"weight\", when(col(\"label\") == 1.0, class_weights[1.0]).otherwise(class_weights[0.0]))\n",
        "\n",
        "# Separación de conjuntos\n",
        "(train_weighted, test_weighted) = df_weighted.randomSplit([0.8, 0.2], seed=42)\n",
        "train_weighted.cache()\n",
        "test_weighted.cache()\n",
        "\n",
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", seed=42, weightCol=\"weight\")\n",
        "\n",
        "# --- Pipeline 1: Con Regresión Logística ---\n",
        "pipeline_lr = Pipeline(stages=[\n",
        "    tokenizer_text, stopwords_text, hashingtf_text,\n",
        "    index_category, ohe_category,\n",
        "    assembler,\n",
        "    lr\n",
        "])\n",
        "\n",
        "# --- Pipeline 2: Con Random Forest ---\n",
        "pipeline_rf = Pipeline(stages=[\n",
        "    tokenizer_text, stopwords_text, hashingtf_text,\n",
        "    index_category, ohe_category,\n",
        "    assembler,\n",
        "    rf\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYqJtNaxhqnv"
      },
      "source": [
        "### 3.2. Entrenamiento de los modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yes3b14W-nNH",
        "outputId": "0bf2d300-4a3a-4d84-8f85-2f64a3bc6d23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando Modelo 1: Regresión Lineal\n"
          ]
        }
      ],
      "source": [
        "print(\"Entrenando Modelo 1: Regresión Lineal\")\n",
        "model_lr = pipeline_lr.fit(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6Jl05Wftqpd",
        "outputId": "9e7048a0-fba1-4328-a560-cb9408ae5bd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Entrenando Modelo 2: Random Forest\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nEntrenando Modelo 2: Random Forest\")\n",
        "model_rf = pipeline_rf.fit(train_weighted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJV9XfFWR9Af"
      },
      "source": [
        "### 3.3. Generar predicciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9OKZfSygohF",
        "outputId": "abf2f7cd-4f56-4c85-9ecb-8cf1c577c119"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicciones Modelo 1: Regresión Logística\n",
            "Predicciones Modelo 2: Random Forest\n"
          ]
        }
      ],
      "source": [
        "print(\"Predicciones Modelo 1: Regresión Logística\")\n",
        "predictions_lr = model_lr.transform(test)\n",
        "print(\"Predicciones Modelo 2: Random Forest\")\n",
        "predictions_rf = model_rf.transform(test_weighted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njq1JsYhgeBi"
      },
      "source": [
        "## 4. Evaluación de los modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdvqQQOSgdB2",
        "outputId": "f2cf728f-4586-4f95-9b61-61c4d90a5de7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> Evaluación Regresión Logística\n",
            "Accuracy (LR): 0.8814\n",
            "F1-Score (LR): 0.8525\n",
            "Precision (para Negativos) (LR): 0.6397\n",
            "Recall (para Positivos) (LR): 0.9838\n",
            "\n",
            "Matriz de Confusión (LR):\n",
            "+-----+----------+------+\n",
            "|label|prediction| count|\n",
            "+-----+----------+------+\n",
            "|  0.0|       0.0|  9625|\n",
            "|  0.0|       1.0| 40234|\n",
            "|  1.0|       0.0|  5420|\n",
            "|  1.0|       1.0|329689|\n",
            "+-----+----------+------+\n",
            "\n",
            "\n",
            ">> Evaluación Random Forest\n",
            "Accuracy (RF): 0.8350\n",
            "F1-Score (RF): 0.8301\n",
            "Precision (para Negativos) (RF): 0.3426\n",
            "Recall (para Positivos) (RF): 0.9149\n",
            "\n",
            "Matriz de Confusión (RF):\n",
            "+-----+----------+------+\n",
            "|label|prediction| count|\n",
            "+-----+----------+------+\n",
            "|  0.0|       0.0| 14864|\n",
            "|  0.0|       1.0| 34995|\n",
            "|  1.0|       0.0| 28525|\n",
            "|  1.0|       1.0|306584|\n",
            "+-----+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import col, sum, when\n",
        "\n",
        "print(\">> Evaluación Regresión Logística\")\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
        "\n",
        "acc_lr = evaluator.setMetricName(\"accuracy\").evaluate(predictions_lr)\n",
        "print(f\"Accuracy (LR): {acc_lr:.4f}\")\n",
        "\n",
        "f1_lr = evaluator.setMetricName(\"f1\").evaluate(predictions_lr)\n",
        "print(f\"F1-Score (LR): {f1_lr:.4f}\")\n",
        "\n",
        "prec_lr = evaluator.setMetricName(\"precisionByLabel\").setMetricLabel(0.0).evaluate(predictions_lr)\n",
        "print(f\"Precision (para Negativos) (LR): {prec_lr:.4f}\")\n",
        "\n",
        "recall_lr = evaluator.setMetricName(\"recallByLabel\").setMetricLabel(1.0).evaluate(predictions_lr)\n",
        "print(f\"Recall (para Positivos) (LR): {recall_lr:.4f}\")\n",
        "\n",
        "print(\"\\nMatriz de Confusión (LR):\")\n",
        "confusion_matrix_lr = predictions_lr.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\")\n",
        "confusion_matrix_lr.show()\n",
        "\n",
        "print(\"\\n>> Evaluación Random Forest\")\n",
        "\n",
        "acc_rf = evaluator.setMetricName(\"accuracy\").evaluate(predictions_rf)\n",
        "print(f\"Accuracy (RF): {acc_rf:.4f}\")\n",
        "\n",
        "f1_rf = evaluator.setMetricName(\"f1\").evaluate(predictions_rf)\n",
        "print(f\"F1-Score (RF): {f1_rf:.4f}\")\n",
        "\n",
        "prec_rf = evaluator.setMetricName(\"precisionByLabel\").setMetricLabel(0.0).evaluate(predictions_rf)\n",
        "print(f\"Precision (para Negativos) (RF): {prec_rf:.4f}\")\n",
        "\n",
        "recall_rf = evaluator.setMetricName(\"recallByLabel\").setMetricLabel(1.0).evaluate(predictions_rf)\n",
        "print(f\"Recall (para Positivos) (RF): {recall_rf:.4f}\")\n",
        "\n",
        "print(\"\\nMatriz de Confusión (RF):\")\n",
        "confusion_matrix_rf = predictions_rf.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\")\n",
        "confusion_matrix_rf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TqjWWbYhR7L"
      },
      "source": [
        "## 5. Visualización\n",
        "\n",
        "### 5.1. Exportación de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYy3eblrZkMK",
        "outputId": "d12ff635-3836-4625-fadf-0f2f31890554"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exportando CSVs para Tableau...\n",
            "Archivo de Categorías guardado en: /content/gdrive/MyDrive/AI_data_set/AmazonBooksReviews/tableau_category.csv\n",
            "Archivo de Autores guardado en: /content/gdrive/MyDrive/AI_data_set/AmazonBooksReviews/tableau_author.csv\n"
          ]
        }
      ],
      "source": [
        "def aggregate_model_performance(predictions_df, model_name, group_by_col):\n",
        "\n",
        "    df_agg = predictions_df.groupBy(group_by_col) \\\n",
        "        .agg(\n",
        "            count(\"*\").alias(\"total_de_reseñas\"),\n",
        "            sum(when((col(\"label\") == 0.0) & (col(\"prediction\") == 0.0), 1).otherwise(0)).alias(\"VN\"),\n",
        "            sum(when((col(\"label\") == 1.0) & (col(\"prediction\") == 0.0), 1).otherwise(0)).alias(\"FP\"),\n",
        "            sum(when((col(\"label\") == 0.0) & (col(\"prediction\") == 1.0), 1).otherwise(0)).alias(\"FN\"),\n",
        "            sum(when((col(\"label\") == 1.0) & (col(\"prediction\") == 1.0), 1).otherwise(0)).alias(\"VP\")\n",
        "        ) \\\n",
        "        .withColumn(\"modelo\", lit(model_name))\n",
        "\n",
        "    return df_agg\n",
        "\n",
        "agg_lr_cat = aggregate_model_performance(predictions_lr, \"Regresión Logística\", \"category\")\n",
        "agg_rf_cat = aggregate_model_performance(predictions_rf, \"Random Forest\", \"category\")\n",
        "\n",
        "df_comparacion_categoria = agg_lr_cat.union(agg_rf_cat)\n",
        "\n",
        "agg_lr_auth = aggregate_model_performance(predictions_lr, \"Regresión Logística\", \"first_author\")\n",
        "agg_rf_auth = aggregate_model_performance(predictions_rf, \"Random Forest\", \"first_author\")\n",
        "\n",
        "df_comparacion_autor = agg_lr_auth.union(agg_rf_auth)\n",
        "\n",
        "print(\"Exportando CSVs para Tableau...\")\n",
        "path_cat = \"/content/gdrive/MyDrive/AI_data_set/AmazonBooksReviews/tableau_category.csv\"\n",
        "path_auth = \"/content/gdrive/MyDrive/AI_data_set/AmazonBooksReviews/tableau_author.csv\"\n",
        "\n",
        "# Exportar directamente a CSV usando PySpark\n",
        "df_comparacion_categoria.coalesce(1).write.csv(path_cat, header=True, mode=\"overwrite\")\n",
        "print(f\"Archivo de Categorías guardado en: {path_cat}\")\n",
        "\n",
        "df_comparacion_autor.coalesce(1).write.csv(path_auth, header=True, mode=\"overwrite\")\n",
        "print(f\"Archivo de Autores guardado en: {path_auth}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTKLRuG6mqc1"
      },
      "source": [
        "## Conclusiones\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ag4PmETO2NB"
      },
      "source": [
        "## Referencias\n",
        "\n",
        "1.  Bekheet, M. (2022, septiembre 13). *Amazon Books Reviews*. https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews/data\n",
        "2. Sharma, S. (2023, diciembre 28). *Amazon Books Review (EDA + Sentiment-Analysis)*. https://www.kaggle.com/code/shubham2703/amazon-books-review-eda-sentiment-analysis\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
